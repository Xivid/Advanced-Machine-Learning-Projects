{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training data...\n"
     ]
    }
   ],
   "source": [
    "print('Load training data...')\n",
    "df_x_train = pd.read_csv('X_train.csv', header=0, index_col = 0)\n",
    "df_y_train = pd.read_csv('y_train.csv', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x990</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.524480</td>\n",
       "      <td>-0.718917</td>\n",
       "      <td>0.827537</td>\n",
       "      <td>-2.336564</td>\n",
       "      <td>1.905992</td>\n",
       "      <td>-1.424985</td>\n",
       "      <td>-5.622933</td>\n",
       "      <td>-0.739429</td>\n",
       "      <td>0.334367</td>\n",
       "      <td>-0.245799</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.214350</td>\n",
       "      <td>-4.083583</td>\n",
       "      <td>-1.240234</td>\n",
       "      <td>1.581522</td>\n",
       "      <td>-3.147444</td>\n",
       "      <td>0.423618</td>\n",
       "      <td>2.387999</td>\n",
       "      <td>1.784247</td>\n",
       "      <td>-1.689361</td>\n",
       "      <td>-1.586569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.561814</td>\n",
       "      <td>-0.115757</td>\n",
       "      <td>-0.113303</td>\n",
       "      <td>-0.322508</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>0.042634</td>\n",
       "      <td>-0.319990</td>\n",
       "      <td>-0.066997</td>\n",
       "      <td>0.281196</td>\n",
       "      <td>-0.064463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262083</td>\n",
       "      <td>-0.437542</td>\n",
       "      <td>0.300902</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>-0.537463</td>\n",
       "      <td>0.455991</td>\n",
       "      <td>-0.378800</td>\n",
       "      <td>-0.536470</td>\n",
       "      <td>-0.810315</td>\n",
       "      <td>-0.021378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.547026</td>\n",
       "      <td>-0.045593</td>\n",
       "      <td>1.016072</td>\n",
       "      <td>-0.068002</td>\n",
       "      <td>-0.670472</td>\n",
       "      <td>-0.551299</td>\n",
       "      <td>-0.550926</td>\n",
       "      <td>0.393147</td>\n",
       "      <td>1.022467</td>\n",
       "      <td>-0.113551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064608</td>\n",
       "      <td>-0.361322</td>\n",
       "      <td>-0.440028</td>\n",
       "      <td>0.278972</td>\n",
       "      <td>-0.570960</td>\n",
       "      <td>-0.708099</td>\n",
       "      <td>-0.025025</td>\n",
       "      <td>0.552631</td>\n",
       "      <td>-1.365591</td>\n",
       "      <td>-0.584266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.939258</td>\n",
       "      <td>-0.284554</td>\n",
       "      <td>1.276007</td>\n",
       "      <td>-0.500731</td>\n",
       "      <td>1.088817</td>\n",
       "      <td>-0.897736</td>\n",
       "      <td>-1.530660</td>\n",
       "      <td>-0.952914</td>\n",
       "      <td>1.157809</td>\n",
       "      <td>0.149595</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.341105</td>\n",
       "      <td>-1.663250</td>\n",
       "      <td>-1.224091</td>\n",
       "      <td>0.617387</td>\n",
       "      <td>-0.964099</td>\n",
       "      <td>-0.034949</td>\n",
       "      <td>0.157197</td>\n",
       "      <td>0.137123</td>\n",
       "      <td>-0.165070</td>\n",
       "      <td>-0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.386835</td>\n",
       "      <td>-0.143997</td>\n",
       "      <td>0.506509</td>\n",
       "      <td>-0.648928</td>\n",
       "      <td>-0.614121</td>\n",
       "      <td>0.211504</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>-0.272372</td>\n",
       "      <td>-0.215564</td>\n",
       "      <td>0.070822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083148</td>\n",
       "      <td>0.568495</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.317415</td>\n",
       "      <td>-0.163551</td>\n",
       "      <td>-0.240090</td>\n",
       "      <td>-0.270020</td>\n",
       "      <td>-0.296239</td>\n",
       "      <td>-0.722527</td>\n",
       "      <td>0.986404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x0        x1        x2        x3        x4        x5        x6  \\\n",
       "id                                                                         \n",
       "0  -4.524480 -0.718917  0.827537 -2.336564  1.905992 -1.424985 -5.622933   \n",
       "1  -0.561814 -0.115757 -0.113303 -0.322508 -0.080855  0.042634 -0.319990   \n",
       "2  -0.547026 -0.045593  1.016072 -0.068002 -0.670472 -0.551299 -0.550926   \n",
       "3  -1.939258 -0.284554  1.276007 -0.500731  1.088817 -0.897736 -1.530660   \n",
       "4  -0.386835 -0.143997  0.506509 -0.648928 -0.614121  0.211504  0.025600   \n",
       "\n",
       "          x7        x8        x9    ...         x990      x991      x992  \\\n",
       "id                                  ...                                    \n",
       "0  -0.739429  0.334367 -0.245799    ...    -3.214350 -4.083583 -1.240234   \n",
       "1  -0.066997  0.281196 -0.064463    ...    -0.262083 -0.437542  0.300902   \n",
       "2   0.393147  1.022467 -0.113551    ...     0.064608 -0.361322 -0.440028   \n",
       "3  -0.952914  1.157809  0.149595    ...    -2.341105 -1.663250 -1.224091   \n",
       "4  -0.272372 -0.215564  0.070822    ...     0.083148  0.568495  0.050980   \n",
       "\n",
       "        x993      x994      x995      x996      x997      x998      x999  \n",
       "id                                                                        \n",
       "0   1.581522 -3.147444  0.423618  2.387999  1.784247 -1.689361 -1.586569  \n",
       "1   0.502415 -0.537463  0.455991 -0.378800 -0.536470 -0.810315 -0.021378  \n",
       "2   0.278972 -0.570960 -0.708099 -0.025025  0.552631 -1.365591 -0.584266  \n",
       "3   0.617387 -0.964099 -0.034949  0.157197  0.137123 -0.165070 -0.740363  \n",
       "4   0.317415 -0.163551 -0.240090 -0.270020 -0.296239 -0.722527  0.986404  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y\n",
       "id     \n",
       "0   1.0\n",
       "1   0.0\n",
       "2   1.0\n",
       "3   1.0\n",
       "4   1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training and validation dataset\n"
     ]
    }
   ],
   "source": [
    "print('Splitting into training and validation dataset')\n",
    "X = df_x_train.values\n",
    "y = df_y_train['y'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2) # , random_state = 19960503)\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\ttraining's multi_logloss: 0.665169\ttraining's BMAC: 0.759616\tvalid_1's multi_logloss: 0.673457\tvalid_1's BMAC: 0.57915\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's multi_logloss: 0.639849\ttraining's BMAC: 0.808987\tvalid_1's multi_logloss: 0.654788\tvalid_1's BMAC: 0.601451\n",
      "[3]\ttraining's multi_logloss: 0.615877\ttraining's BMAC: 0.839199\tvalid_1's multi_logloss: 0.637204\tvalid_1's BMAC: 0.604237\n",
      "[4]\ttraining's multi_logloss: 0.593564\ttraining's BMAC: 0.851697\tvalid_1's multi_logloss: 0.62191\tvalid_1's BMAC: 0.619788\n",
      "[5]\ttraining's multi_logloss: 0.573463\ttraining's BMAC: 0.857855\tvalid_1's multi_logloss: 0.605925\tvalid_1's BMAC: 0.63482\n",
      "[6]\ttraining's multi_logloss: 0.554301\ttraining's BMAC: 0.867036\tvalid_1's multi_logloss: 0.592816\tvalid_1's BMAC: 0.639809\n",
      "[7]\ttraining's multi_logloss: 0.536118\ttraining's BMAC: 0.874134\tvalid_1's multi_logloss: 0.581444\tvalid_1's BMAC: 0.645526\n",
      "[8]\ttraining's multi_logloss: 0.519481\ttraining's BMAC: 0.876936\tvalid_1's multi_logloss: 0.569922\tvalid_1's BMAC: 0.65314\n",
      "[9]\ttraining's multi_logloss: 0.503611\ttraining's BMAC: 0.878676\tvalid_1's multi_logloss: 0.56005\tvalid_1's BMAC: 0.64417\n",
      "[10]\ttraining's multi_logloss: 0.488345\ttraining's BMAC: 0.885219\tvalid_1's multi_logloss: 0.551047\tvalid_1's BMAC: 0.645832\n",
      "[11]\ttraining's multi_logloss: 0.474002\ttraining's BMAC: 0.892106\tvalid_1's multi_logloss: 0.543167\tvalid_1's BMAC: 0.652979\n",
      "[12]\ttraining's multi_logloss: 0.460804\ttraining's BMAC: 0.89708\tvalid_1's multi_logloss: 0.535275\tvalid_1's BMAC: 0.657006\n",
      "[13]\ttraining's multi_logloss: 0.447947\ttraining's BMAC: 0.900129\tvalid_1's multi_logloss: 0.528698\tvalid_1's BMAC: 0.652539\n",
      "[14]\ttraining's multi_logloss: 0.435756\ttraining's BMAC: 0.905387\tvalid_1's multi_logloss: 0.521176\tvalid_1's BMAC: 0.656405\n",
      "[15]\ttraining's multi_logloss: 0.424702\ttraining's BMAC: 0.907596\tvalid_1's multi_logloss: 0.515873\tvalid_1's BMAC: 0.654275\n",
      "[16]\ttraining's multi_logloss: 0.413413\ttraining's BMAC: 0.913661\tvalid_1's multi_logloss: 0.50921\tvalid_1's BMAC: 0.651964\n",
      "[17]\ttraining's multi_logloss: 0.402553\ttraining's BMAC: 0.915867\tvalid_1's multi_logloss: 0.502909\tvalid_1's BMAC: 0.650742\n",
      "[18]\ttraining's multi_logloss: 0.392445\ttraining's BMAC: 0.922166\tvalid_1's multi_logloss: 0.498093\tvalid_1's BMAC: 0.649988\n",
      "[19]\ttraining's multi_logloss: 0.382955\ttraining's BMAC: 0.925782\tvalid_1's multi_logloss: 0.493153\tvalid_1's BMAC: 0.65121\n",
      "[20]\ttraining's multi_logloss: 0.374044\ttraining's BMAC: 0.927306\tvalid_1's multi_logloss: 0.489725\tvalid_1's BMAC: 0.650742\n",
      "[21]\ttraining's multi_logloss: 0.365418\ttraining's BMAC: 0.928599\tvalid_1's multi_logloss: 0.485125\tvalid_1's BMAC: 0.657556\n",
      "[22]\ttraining's multi_logloss: 0.35709\ttraining's BMAC: 0.931258\tvalid_1's multi_logloss: 0.480233\tvalid_1's BMAC: 0.655551\n",
      "[23]\ttraining's multi_logloss: 0.34914\ttraining's BMAC: 0.933835\tvalid_1's multi_logloss: 0.476509\tvalid_1's BMAC: 0.644593\n",
      "[24]\ttraining's multi_logloss: 0.341464\ttraining's BMAC: 0.939199\tvalid_1's multi_logloss: 0.47193\tvalid_1's BMAC: 0.656648\n",
      "[25]\ttraining's multi_logloss: 0.334161\ttraining's BMAC: 0.939072\tvalid_1's multi_logloss: 0.46949\tvalid_1's BMAC: 0.64987\n",
      "[26]\ttraining's multi_logloss: 0.326949\ttraining's BMAC: 0.936021\tvalid_1's multi_logloss: 0.466311\tvalid_1's BMAC: 0.65343\n",
      "[27]\ttraining's multi_logloss: 0.319797\ttraining's BMAC: 0.938827\tvalid_1's multi_logloss: 0.462756\tvalid_1's BMAC: 0.65174\n",
      "[28]\ttraining's multi_logloss: 0.312836\ttraining's BMAC: 0.940342\tvalid_1's multi_logloss: 0.459993\tvalid_1's BMAC: 0.650958\n",
      "[29]\ttraining's multi_logloss: 0.306417\ttraining's BMAC: 0.943023\tvalid_1's multi_logloss: 0.457543\tvalid_1's BMAC: 0.65992\n",
      "[30]\ttraining's multi_logloss: 0.300327\ttraining's BMAC: 0.944893\tvalid_1's multi_logloss: 0.454769\tvalid_1's BMAC: 0.660235\n",
      "[31]\ttraining's multi_logloss: 0.294506\ttraining's BMAC: 0.94441\tvalid_1's multi_logloss: 0.452247\tvalid_1's BMAC: 0.661323\n",
      "[32]\ttraining's multi_logloss: 0.288793\ttraining's BMAC: 0.946502\tvalid_1's multi_logloss: 0.450332\tvalid_1's BMAC: 0.658392\n",
      "[33]\ttraining's multi_logloss: 0.283381\ttraining's BMAC: 0.94789\tvalid_1's multi_logloss: 0.448749\tvalid_1's BMAC: 0.658392\n",
      "[34]\ttraining's multi_logloss: 0.278171\ttraining's BMAC: 0.949425\tvalid_1's multi_logloss: 0.446713\tvalid_1's BMAC: 0.65948\n",
      "[35]\ttraining's multi_logloss: 0.272931\ttraining's BMAC: 0.950223\tvalid_1's multi_logloss: 0.444939\tvalid_1's BMAC: 0.66535\n",
      "[36]\ttraining's multi_logloss: 0.26772\ttraining's BMAC: 0.955364\tvalid_1's multi_logloss: 0.443627\tvalid_1's BMAC: 0.664415\n",
      "[37]\ttraining's multi_logloss: 0.262901\ttraining's BMAC: 0.958762\tvalid_1's multi_logloss: 0.442207\tvalid_1's BMAC: 0.659767\n",
      "[38]\ttraining's multi_logloss: 0.258156\ttraining's BMAC: 0.958983\tvalid_1's multi_logloss: 0.440332\tvalid_1's BMAC: 0.650966\n",
      "[39]\ttraining's multi_logloss: 0.253162\ttraining's BMAC: 0.960603\tvalid_1's multi_logloss: 0.43902\tvalid_1's BMAC: 0.655614\n",
      "[40]\ttraining's multi_logloss: 0.248714\ttraining's BMAC: 0.962241\tvalid_1's multi_logloss: 0.438818\tvalid_1's BMAC: 0.650832\n",
      "[41]\ttraining's multi_logloss: 0.243889\ttraining's BMAC: 0.963051\tvalid_1's multi_logloss: 0.437198\tvalid_1's BMAC: 0.656702\n",
      "[42]\ttraining's multi_logloss: 0.239691\ttraining's BMAC: 0.962588\tvalid_1's multi_logloss: 0.43654\tvalid_1's BMAC: 0.671373\n",
      "[43]\ttraining's multi_logloss: 0.235208\ttraining's BMAC: 0.964575\tvalid_1's multi_logloss: 0.43632\tvalid_1's BMAC: 0.666906\n",
      "[44]\ttraining's multi_logloss: 0.230869\ttraining's BMAC: 0.965152\tvalid_1's multi_logloss: 0.434817\tvalid_1's BMAC: 0.661971\n",
      "[45]\ttraining's multi_logloss: 0.226735\ttraining's BMAC: 0.96737\tvalid_1's multi_logloss: 0.433875\tvalid_1's BMAC: 0.654258\n",
      "[46]\ttraining's multi_logloss: 0.222592\ttraining's BMAC: 0.965833\tvalid_1's multi_logloss: 0.433118\tvalid_1's BMAC: 0.659814\n",
      "[47]\ttraining's multi_logloss: 0.218741\ttraining's BMAC: 0.968976\tvalid_1's multi_logloss: 0.432868\tvalid_1's BMAC: 0.654258\n",
      "[48]\ttraining's multi_logloss: 0.215044\ttraining's BMAC: 0.968733\tvalid_1's multi_logloss: 0.432277\tvalid_1's BMAC: 0.651633\n",
      "[49]\ttraining's multi_logloss: 0.211471\ttraining's BMAC: 0.969774\tvalid_1's multi_logloss: 0.43221\tvalid_1's BMAC: 0.649323\n",
      "[50]\ttraining's multi_logloss: 0.208011\ttraining's BMAC: 0.971414\tvalid_1's multi_logloss: 0.433331\tvalid_1's BMAC: 0.647921\n",
      "[51]\ttraining's multi_logloss: 0.204602\ttraining's BMAC: 0.972581\tvalid_1's multi_logloss: 0.433964\tvalid_1's BMAC: 0.646231\n",
      "[52]\ttraining's multi_logloss: 0.201305\ttraining's BMAC: 0.972812\tvalid_1's multi_logloss: 0.433488\tvalid_1's BMAC: 0.654879\n",
      "[53]\ttraining's multi_logloss: 0.198162\ttraining's BMAC: 0.973873\tvalid_1's multi_logloss: 0.433449\tvalid_1's BMAC: 0.641449\n",
      "[54]\ttraining's multi_logloss: 0.195049\ttraining's BMAC: 0.974914\tvalid_1's multi_logloss: 0.433261\tvalid_1's BMAC: 0.646851\n",
      "[55]\ttraining's multi_logloss: 0.192007\ttraining's BMAC: 0.976091\tvalid_1's multi_logloss: 0.433446\tvalid_1's BMAC: 0.644074\n",
      "[56]\ttraining's multi_logloss: 0.188806\ttraining's BMAC: 0.978413\tvalid_1's multi_logloss: 0.433487\tvalid_1's BMAC: 0.638671\n",
      "[57]\ttraining's multi_logloss: 0.185553\ttraining's BMAC: 0.97876\tvalid_1's multi_logloss: 0.434284\tvalid_1's BMAC: 0.642698\n",
      "[58]\ttraining's multi_logloss: 0.182204\ttraining's BMAC: 0.978401\tvalid_1's multi_logloss: 0.434124\tvalid_1's BMAC: 0.636514\n",
      "[59]\ttraining's multi_logloss: 0.179239\ttraining's BMAC: 0.977928\tvalid_1's multi_logloss: 0.434643\tvalid_1's BMAC: 0.645629\n",
      "[60]\ttraining's multi_logloss: 0.176287\ttraining's BMAC: 0.978159\tvalid_1's multi_logloss: 0.434233\tvalid_1's BMAC: 0.641009\n",
      "[61]\ttraining's multi_logloss: 0.173408\ttraining's BMAC: 0.98003\tvalid_1's multi_logloss: 0.434065\tvalid_1's BMAC: 0.644101\n",
      "[62]\ttraining's multi_logloss: 0.170806\ttraining's BMAC: 0.980492\tvalid_1's multi_logloss: 0.434951\tvalid_1's BMAC: 0.647814\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's multi_logloss: 0.239691\ttraining's BMAC: 0.962588\tvalid_1's multi_logloss: 0.43654\tvalid_1's BMAC: 0.671373\n"
     ]
    }
   ],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclassova',\n",
    "    'is_unbalance': True,\n",
    "    #'metric': {'l1', 'l2'},\n",
    "    'num_class': 3,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "def custom_accuracy(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    n = len(labels)\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(np.argmax([preds[i], preds[n + i], preds[2*n + i]]))\n",
    "    return 'BMAC', balanced_accuracy_score(labels, results), True\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                feval=custom_accuracy,\n",
    "                valid_sets={lgb_train, lgb_eval},\n",
    "                early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load testing data...\n"
     ]
    }
   ],
   "source": [
    "print('Load testing data...')\n",
    "df_x_test = pd.read_csv('X_test.csv', header=0, index_col = 0)\n",
    "# X_test = df_x_test[selected_features].values\n",
    "X_test = df_x_test.values\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "y_pred\n",
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write(\"id,y\\n\")\n",
    "for i,x in enumerate(y_pred):\n",
    "    f.write(\"{},{}\\n\".format(i,np.argmax(x)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 571\n",
      "1: 2876\n",
      "2: 653\n"
     ]
    }
   ],
   "source": [
    "a = [np.argmax(x) for x in y_pred]\n",
    "print(\"0: {}\".format(len([x for x in a if x == 0])))\n",
    "print(\"1: {}\".format(len([x for x in a if x == 1])))      \n",
    "print(\"2: {}\".format(len([x for x in a if x == 2])))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
